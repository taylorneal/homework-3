---
title: "ECO 395 Homework 3: Taylor Neal"
output: rmarkdown::github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(rsample) 
library(modelr)
library(randomForest)
library(gbm)
library(ggmap)
#library(knitr)
#library(caret)
#library(glmnet)
#library(lubridate)
#library(ROCR)
#library(foreach)
#library(mosaic)


dengue <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-3/master/data/dengue.csv", header = TRUE)
grn_build <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-3/master/data/greenbuildings.csv", header = TRUE)
CA_housing <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-3/master/data/CAhousing.csv", header = TRUE)

set.seed(9)
```
## 1) What Causes What?

##### Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city.)

The approached suggested above would not account for the underlying levels of crime associated with different cities. Cities with higher rates of crime (due to demographics, regional characteristics, etc.) could reasonably be expected to hire larger police forces in attempt to address crime issues. Thus, the suggested simple regression would (by not accounting for differences in underlying crime levels) not tell us anything about the causal impact of police on crime rates.

##### How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2”, from the researchers' paper.

The researchers from UPenn were able to isolate the impact of police on crime by observing different levels of policing activity in the same city (Washington, D.C.). This increased police activity (associated with a higher terrorism alert level) was unrelated to crime levels in the city and so these high-alert days could be compared to non-alert days to tease out the causal impact of policing activity on crime levels. In Table 2, they found that the regression coefficient on the high-alert indicator variable was negative (and statistically significant at a 5% level) which indicates that the higher level of policing activity for high-alert days did reduce total daily crimes.

##### Why did they have to control for Metro ridership? What was that trying to capture?

Controlling for Metro ridership served as a way of ensuring that the high-alert days were not generally lowering activity levels in the city (and thereby causing crime rates to fall). In this way, they were able to control for the impact of high-alert days dissuading residents and tourists from moving about the city as usual. So, controlling for Metro ridership allowed the researchers to more accurately capture the causal impact of policing activity itself on crime levels. 

##### Below I am showing you "Table 4" from the researchers' paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?

In Table 4, the researchers are continuing to model total daily number of crimes in Washington, D.C.; however, they are separating those crimes reported in district 1 (the National Mall - area near many of the most important federal government buildings) and those reported in the rest of the city. The high-alert days increase police activity generally throughout the city, but with a greater focus/increase in the National Mall district. Thus, the finding that crime decreases are greatest in magnitude in district 1 (coefficient of -2.621) on high-alert days (and statistically significant at a 1% level) provides additional support to the conclusion that higher levels of policing activity cause crime rates to fall. For crimes reported in the rest of the city, the regression still finds that high-alert days decrease crimes reported, but at a much lower magnitude (coefficient of -0.571) that is not statistically significant at a 5% level. 

## 2) Tree Modeling: Dengue Cases

Here we seek to build tree models utilizing three different methodologies (CART, random forests and gradient-boosted trees) in order to predict weekly dengue cases in San Juan, Puerto Rico and Iquitos, Peru based on the provided data set. Prior to any model building, the data was split in order to randomly reserve 20% of the observations for testing and comparing the performance of the three models. For each of the three methodologies, we model weekly dengue cases based on city (San Juan or Iquitos), season (spring, summer, fall, or winter), specific humidity, average diurnal temperature range, precipitation amount and maximum air temperature.

```{r CART-tree, echo = FALSE, fig.align = 'center'}

dengue$city = as.factor(dengue$city)
dengue$season = as.factor(dengue$season)
dengue$total_cases = as.numeric(dengue$total_cases)
dengue <- dengue[!is.na(dengue$tdtr_k),]
dengue <- subset(dengue, select = c(-4, -5, -6, -7))

dengue_split = initial_split(dengue, prop = 0.8)
dengue_train = training(dengue_split)
dengue_test = testing(dengue_split)

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp = cp_opt)
}

load.tree = rpart(total_cases ~ city + season + tdtr_k + specific_humidity + 
                    precipitation_amt + max_air_temp_k, 
                  data = dengue_train, 
                  control = rpart.control(cp = 0.0001, minsplit = 5))

load.tree_pruned = prune_1se(load.tree)
rpart.plot(load.tree_pruned, digits = -5, type = 4, extra = 1)

```

Utilizing the CART methodology we greedily and recursively grow our tree to make deviance as small as possible (for this we used a complexity parameter of 0.0001 and minimum split of 5 observations). The resulting candidate trees were pruned back using cross validation taking the simplest tree within 1 standard error of the minimum cross-validated error. This tree (above) turns out to be very simple but it does just as well as the deeper trees at modeling our training set. 

```{r partial-humidity, echo = FALSE, fig.align = 'center'}

load.forest = randomForest(total_cases ~ city + season + tdtr_k + 
                             specific_humidity + precipitation_amt + 
                             max_air_temp_k,
                           data = dengue_train, importance = TRUE, 
                           na.action = na.roughfix)

boost = gbm(total_cases ~ city + season + tdtr_k + specific_humidity + 
              precipitation_amt + max_air_temp_k, distribution = 'gaussian',
            data = dengue_train, interaction.depth = 4, n.trees = 500, 
            shrinkage = .03)

CART_rmse = modelr::rmse(load.tree_pruned, dengue_test)
forest_rmse = modelr::rmse(load.forest, dengue_test)
boosted_rmse = modelr::rmse(boost, dengue_test)

partialPlot(load.forest, dengue_test, 'specific_humidity', las = 1)

```

In the case of gradient-boosted trees, we assumed a Gaussian distribution. Additional parameters included an interaction depth of 4, 500 trees, and a shrinkage factor of 0.03. This resulted in an improvement of RMSE (`r round(boosted_rmse, 2)`) of the gradient-boosted tree model when compared to the final pruned CART tree (with RMSE of `r round(CART_rmse, 2)`). However, the "out-of-the-box" random forest (with 500 trees and the square root of the number of variables used as the number of randomly sampled variables) had the lowest RMSE (`r round(forest_rmse, 2)`) when tested against the 20% of our observations which were withheld for model building purposes. The plot above shows the random forest's partial dependence plot for specific humidity. 

```{r partial-precipitation, echo = FALSE, fig.align = 'center'}

partialPlot(load.forest, dengue_test, 'precipitation_amt', las = 1)

```

The partial dependence plot for precipitation amount (above) has somewhat strange behavior for very low and very high amounts but this is likely due to the limited number of data points at those extremes. Overall, we see that precipitation amount appears to increase the expected number of dengue cases (likely because the standing water provides ample breeding grounds for mosquito). But at very high levels of precipitation, we might expect the number of dengue cases to fall because more people are inside avoiding the rain.

```{r partial-max-temp, echo = FALSE, fig.align = 'center'}

partialPlot(load.forest, dengue_test, 'max_air_temp_k', las = 1)

```

The partial dependence plot for maximum air temperature (above) has odd behavior at very low temperatures (likely due to few observations), but if we ignore the area past the last tick mark on the left we find interesting partial dependence behavior. We see a near vertical line around 303K which appears to demarcate a temperature where that max temperature is hot enough for mosquitos to proliferate and spread more dengue cases. However, above this certain temperature, the maximum does not appear to add significantly to cases as it continues getting even hotter. 

## 3) Predictive Model Building: Green Certification

TBU 

## 4) Predictive Model Building: California Housing

In the following exercise, we seek to build a predictive model for median house value in a census tract. To begin, `totalRooms` and `totalBedrooms` were standardized by household (resulting in `rooms_per_household` and `bedrooms_per_household`, respectively). Additionally, a new factor was created by dividing tract population by number of households to arrive at an average household size (`avg_household`). 20% of the tract observations were withheld from the training and model building processes in order to test and estimate out-of-sample accuracy. 

```{r HouseValue-maps, echo = FALSE, fig.show="hold", out.width="50%"}

CA_housing = CA_housing %>% 
  mutate(group = 1, rooms_per_household = totalRooms / households,
         bedrooms_per_household = totalBedrooms / households, 
         avg_household = population / households)

states = map_data("state")
counties = map_data("county")
ca_map = subset(states, region == "california")
ca_counties = subset(counties, region == "california")

ca_base = ggplot(data = ca_map, mapping = aes(x = long, y = lat, group = group)) + 
  coord_fixed(1.3) + geom_polygon(color = "black", fill = "gray") + 
  theme_nothing() + 
  geom_polygon(data = ca_counties, fill = NA, color = "white") +
  geom_polygon(color = "black", fill = NA)

ca_base + geom_point(data = CA_housing, size = .75, mapping = 
                       aes(x = longitude, y = latitude, color = medianHouseValue)) + 
  scale_color_gradient(low = "cyan", high = "dark blue", labels = scales::label_comma()) + 
  theme(legend.position = "right", legend.title = element_text(), legend.text = element_text(size = 8))

CA_split = initial_split(CA_housing, prop = 0.8)
CA_train = training(CA_split)
CA_test = testing(CA_split)

lm_start = lm(medianHouseValue ~ housingMedianAge + medianIncome
              + rooms_per_household + bedrooms_per_household
              + avg_household + population, data = CA_train)

lm_step = step(lm_start, scope = ~(.)^2, trace = 0)

lm_rmse = modelr::rmse(lm_step, CA_test)

ca.forest = randomForest(medianHouseValue ~ housingMedianAge + medianIncome
                         + rooms_per_household + bedrooms_per_household
                         + avg_household + population, mtry = 3, ntree = 200,
                           data = CA_train, importance = TRUE)

rf_rmse = modelr::rmse(ca.forest, CA_test)

CA_housing$PredictedMedianValue = predict(ca.forest, CA_housing)
CA_housing = CA_housing %>% mutate(Residuals = medianHouseValue - PredictedMedianValue)

ca_base + geom_point(data = CA_housing, size = .75, mapping = 
                       aes(x = longitude, y = latitude, color = PredictedMedianValue)) + 
  scale_color_gradient(low = "cyan", high = "dark blue", labels = scales::label_comma()) + 
  theme(legend.position = "right", legend.title = element_text(), legend.text = element_text(size = 8))

```

In the figure above (on the left), we see `medianHouseValue` plotted as a color scale on a map of California and its counties. As we might expect, tracts with higher median house values are concentrated on the coast and near cities. Additionally, in the figure on the right, we find predicted median house values based on our final model plotted on the same map of California. While not identical, the predicted values do appear to map similarly to the actual data when plotted geographically. The final model we utilize to obtain these predictions is a random forest with 200 trees and 3 randomly sampled variables per tree. `medianHouseValue` is modeled based on `housingMedianAge`, `medianIncome`, `rooms_per_household`, `bedrooms_per_household`, `avg_household`, and `population`. The resulting model had an out-of-sample RMSE of `r format(round(rf_rmse, 0), nsmall = 0, big.mark = ",")`. For benchmarking, a step-wise selected linear model was also fit (with the same explanatory variables as above for the random forest and in the space of all pairwise interactions). This resulted in an out-of-sample RMSE of `r format(lm_rmse, nsmall = 0, big.mark = ",")`.

```{r residuals-map, echo = FALSE, fig.align = 'center'}

ca_base + geom_point(data = CA_housing, size = 1, mapping = 
                       aes(x = longitude, y = latitude, color = Residuals)) + 
  scale_color_gradient2(low = "red", mid = "grey", high = "blue", labels = scales::label_comma()) + 
  theme(legend.position = "right", legend.title = element_text(), legend.text = element_text(size = 8))

```

The figure above displays residuals/errors (actual values minus predictions) for the random forest predicted median house values. Values close to zero show up as grey so they blend into the background of the map. The model appears to do fairly well for most census tracts. As we might expect, when the model is undervaluing a given tract, it tends to be along the coast or near a city. And the model overvalues a fair amount of tracts in central California. Attempts were made to improve the model by calculating distance between census tract centroids to utilize the median value of nearby neighbors in predictions, but those attempts ran into computation constraints when calculating distances between all 20,000+ census tracts in California. 